{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# What does the pipeline do?",
   "id": "848a7bb83704e849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T20:53:03.255708Z",
     "start_time": "2024-08-12T20:53:01.565038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "93f2c95d7ae7e6c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessing with a tokenizer",
   "id": "f9a68d46648f87ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T21:06:22.080773Z",
     "start_time": "2024-08-10T21:06:21.736574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ],
   "id": "3deb163754e4c375",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Why Can tokenizer Accept Arguments?\n",
    "`tokenizer` as a Callable Object:\n",
    "In Python, classes can define a special method called __call__. If a class has this method, instances of the class can be called like a function.\n",
    "The AutoTokenizer (or more specifically, the tokenizer class that AutoTokenizer.from_pretrained returns) has a `__call__` method defined, which allows you to pass arguments directly to it as if it were a function."
   ],
   "id": "b651dd7850eba966"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following model contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. \n",
    "\n",
    "For each model input, we’ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "A high-dimensional vector?\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "1. Batch size: The **number of sequences** processed at a time (2 in our example).\n",
    "\n",
    "2. Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "\n",
    "3. Hidden size: The vector dimension of each model input. (768 in our example). The hidden_size refers to the dimensionality (i.e., the number of features or components)"
   ],
   "id": "381163262ea0ea3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "from transformers import AutoModel\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "print(type(outputs['last_hidden_state']))"
   ],
   "id": "fc53697c2c4f7652"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following model has a sequence classification head (to be able to classify the sentences as positive or negative). So, we won’t actually use the AutoModel class, but AutoModelForSequenceClassification",
   "id": "1dea18e90245748c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:35:21.483192Z",
     "start_time": "2024-08-10T23:35:20.631144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)\n",
    "print(outputs.logits)"
   ],
   "id": "a7fa70610cc9cc26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:37:30.826142Z",
     "start_time": "2024-08-10T23:37:30.757212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ],
   "id": "26629b20c5b1ae88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4419e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T23:41:32.828245Z",
     "start_time": "2024-08-10T23:41:32.799150Z"
    }
   },
   "cell_type": "code",
   "source": "model.config.id2label",
   "id": "ed77b308258973cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " the model predicted [0.0402 Negative, 0.9598 Positive] for the first sentence and [0.9995, 0.0005] for the second one",
   "id": "23da874a58e7c7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "## AutoModel\n",
    "- AutoModel class, which is handy when you want to instantiate any model from a checkpoint.\n",
    "- wrappers over model library. \n",
    "- automatically guess the appropriate model architecture for your checkpoint "
   ],
   "id": "96b393927a64fff7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Randomly initializing Bert",
   "id": "b8c61005e125b13d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:44:42.607215Z",
     "start_time": "2024-08-11T00:44:40.947254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ],
   "id": "34ac4e6839879f23",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading a Transformer model that is already trained is simple — we can do this using the from_pretrained() method:\n",
   "id": "d1c8be241a79ba33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T20:53:32.439801Z",
     "start_time": "2024-08-12T20:53:32.013324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "3c6ae1e7c579a68a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "saves two files\n",
    "\n",
    "config.json: architecture\n",
    "\n",
    "pytorch_model.bin: weights"
   ],
   "id": "c8851eb9a82f568c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:53:12.366038Z",
     "start_time": "2024-08-12T21:53:11.149337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import pathlib\n",
    "# cur_path = pathlib.Path(__file__).parent.resolve()\n",
    "# import inspect\n",
    "# cur_path = inspect.getfile(lambda: None) #The code inspect.getfile(lambda: None) returns the path to the file where the lambda function is defined. In a Jupyter Notebook, this will not return the directory of the notebook itself. Instead, it will return a path related to the Jupyter environment, which is not useful for saving files relative to the notebook.\n",
    "# # model.save_pretrained(cur_path)\n",
    "import os\n",
    "cur_path = os.getcwd()\n",
    "print(cur_path)\n",
    "model.save_pretrained(cur_path)"
   ],
   "id": "ce20856458e0f5ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adrianhwang/LLMPower/LLMPower\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "dcad66f4a47872ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading the BERT tokenizer trained with the same checkpoint as BERT ",
   "id": "731a84b662bdc43e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:22:05.911862Z",
     "start_time": "2024-08-12T22:22:03.628979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "bc82907eab3f61df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d3cede423a2423c9716390b0459aff4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b8d717864be4c85994115c5db5d5a91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e63b7df3b8c4ea58df5607b50e2dac6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LLMPower/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint",
   "id": "484d5f2d12e6847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "e74005db4fccdac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:24:28.641863Z",
     "start_time": "2024-08-12T22:24:28.615614Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer(\"Using a Transformer network is simple\")",
   "id": "c96aac43a084f6cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:25:24.080400Z",
     "start_time": "2024-08-12T22:25:24.043098Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.save_pretrained(cur_path)",
   "id": "8d2ffab66fd8bb86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/adrianhwang/LLMPower/LLMPower/tokenizer_config.json',\n",
       " '/Users/adrianhwang/LLMPower/LLMPower/special_tokens_map.json',\n",
       " '/Users/adrianhwang/LLMPower/LLMPower/vocab.txt',\n",
       " '/Users/adrianhwang/LLMPower/LLMPower/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tokenization",
   "id": "ccaffe87bad73f60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:34:59.526461Z",
     "start_time": "2024-08-12T22:34:49.429585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ],
   "id": "8f50922ab34b7fdb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Encoding: from tokenizaer to input_ids",
   "id": "f492bf7197f75746"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:37:25.348665Z",
     "start_time": "2024-08-12T22:37:25.325791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ],
   "id": "688fa92c82b06ed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decoding",
   "id": "383fbc2bdf05ac70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:38:27.615193Z",
     "start_time": "2024-08-12T22:38:27.580864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ],
   "id": "8da297d9b834a717",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Handling Muptiple Sequences",
   "id": "6aafce6a713ac69f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T00:02:25.208075Z",
     "start_time": "2024-08-13T00:02:23.747069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ],
   "id": "71799a0196fda0b3",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(ids)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# This line will fail.\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m model(input_ids)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMPower/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMPower/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMPower/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:883\u001B[0m, in \u001B[0;36mDistilBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    876\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m    878\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m    880\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    881\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 883\u001B[0m distilbert_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistilbert(\n\u001B[1;32m    884\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    885\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m    886\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[1;32m    887\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m    888\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    889\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m    890\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m    891\u001B[0m )\n\u001B[1;32m    892\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m distilbert_output[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, seq_len, dim)\u001B[39;00m\n\u001B[1;32m    893\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m hidden_state[:, \u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, dim)\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMPower/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMPower/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMPower/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:683\u001B[0m, in \u001B[0;36mDistilBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    681\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 683\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001B[1;32m    684\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMPower/lib/python3.12/site-packages/transformers/modeling_utils.py:4665\u001B[0m, in \u001B[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001B[0;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[1;32m   4662\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   4664\u001B[0m \u001B[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001B[39;00m\n\u001B[0;32m-> 4665\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;129;01min\u001B[39;00m input_ids[:, [\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m]]:\n\u001B[1;32m   4666\u001B[0m     warn_string \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   4667\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4668\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4669\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4670\u001B[0m     )\n\u001B[1;32m   4672\u001B[0m     \u001B[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001B[39;00m\n\u001B[1;32m   4673\u001B[0m     \u001B[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001B[39;00m\n",
      "\u001B[0;31mIndexError\u001B[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Why IndexError: too many indices for tensor of dimension 1?\n",
    "the tokenizer didn’t just convert the list of input IDs into a tensor, it added a dimension on top of it"
   ],
   "id": "687f9f0347216c85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T00:08:54.231343Z",
     "start_time": "2024-08-13T00:08:54.162042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ],
   "id": "333e2b498272a4df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:08:59.109643Z",
     "start_time": "2024-08-14T17:08:58.128719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)  # ['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])  #add a dimension\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ],
   "id": "e77bb5f37202e409",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n",
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Batching** is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence",
   "id": "d8100af431617ec6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T00:16:59.726992Z",
     "start_time": "2024-08-13T00:16:59.646671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batched_ids = [ids, ids]\n",
    "batched_ids=torch.tensor(batched_ids)\n",
    "output = model(batched_ids)\n",
    "print(\"Logits:\", output.logits)"
   ],
   "id": "13c585a86240b28e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-2.7276,  2.8789],\n",
      "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T00:41:29.256753Z",
     "start_time": "2024-08-13T00:41:28.348484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ],
   "id": "2cef4c1b221f4484",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Why does the second sentence output inconsistency?\n",
    "Use attention mask to ignore the padding tokens.\n",
    "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to "
   ],
   "id": "7e61b70bc22441b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T01:43:17.499510Z",
     "start_time": "2024-08-13T01:43:16.943288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ],
   "id": "a8897514b1628010",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:43:31.226723Z",
     "start_time": "2024-08-14T17:43:31.096258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence3=\"I hate this so much!\"\n",
    "sequence4=\"I have been waiting for a HuggingFace course my whole life.\"\n",
    "sequence3_tokens = tokenizer.tokenize(sequence3)\n",
    "sequence4_tokens = tokenizer.tokenize(sequence4)\n",
    "sequence3_ids = tokenizer.convert_tokens_to_ids(sequence3_tokens)\n",
    "print(\"Length of sequence3_ids is:\",len(sequence3_ids))\n",
    "sequence4_ids = tokenizer.convert_tokens_to_ids(sequence4_tokens)\n",
    "print(\"Length of sequence4_ids is:\",len(sequence4_ids))\n",
    "sequence3_output = model(torch.tensor([sequence3_ids]))\n",
    "print(\"sequence3's Logits:\", sequence3_output.logits)\n",
    "sequence4_output=model(torch.tensor([sequence4_ids]))\n",
    "print(\"sequence4's Logits:\", sequence4_output.logits)"
   ],
   "id": "960b3b5f4a3d76d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sequence3_ids is: 6\n",
      "Length of sequence4_ids is: 13\n",
      "sequence3's Logits: tensor([[ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n",
      "sequence4's Logits: tensor([[-2.7231,  2.8846]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:48:54.863451Z",
     "start_time": "2024-08-14T17:48:54.579794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#padding sequence3_ids to 13\n",
    "print(\"length of sequence3_ids\", len(sequence3_ids))\n",
    "padding_sequence3_ids = sequence3_ids + [tokenizer.pad_token_id]*(len(sequence4_ids)-len(sequence3_ids))\n",
    "batch34_ids=torch.tensor([padding_sequence3_ids, sequence4_ids]) \n",
    "batch34_output_noAttentionMask=model(batch34_ids)\n",
    "print(\"batch34_output_noAttentionMask's logits are\",batch34_output_noAttentionMask.logits)\n",
    "# attention_mask\n",
    "attention_mask = [\n",
    "    [1]*len(sequence3_ids) + [0]*(len(sequence4_ids)-len(sequence3_ids)),  # [1]*13 + [0]*(13-13) => [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    [1]*len(sequence4_ids)  # [1]*13 => [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "]\n",
    "print(\"attention mask is: \",attention_mask)\n",
    "batch34_output_withAttentionMask=model(batch34_ids, attention_mask=torch.tensor(attention_mask))\n",
    "print(\"batch34_output_withAttentionMask's logits are\",batch34_output_withAttentionMask.logits)\n"
   ],
   "id": "567d43f3de21406a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sequence3_ids 6\n",
      "batch34_output_noAttentionMask's logits are tensor([[ 2.6743, -2.2346],\n",
      "        [-2.7231,  2.8846]], grad_fn=<AddmmBackward0>)\n",
      "attention mask is:  [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "batch34_output_withAttentionMask's logits are tensor([[ 3.1931, -2.6685],\n",
      "        [-2.7231,  2.8846]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Longer sequences\n",
    "\n",
    "Try other models that can handle longer sequences\n",
    " Longformer, LED\n",
    "\n",
    "or truncate the sequences"
   ],
   "id": "1a5e32e5b9502a6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a309d6836c188e68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# sequence = sequence[:max_sequence_length]",
   "id": "41c7ee32dde4bc09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7bde6173ac7fd4e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Put it all together",
   "id": "dea54273210d5c4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ],
   "id": "c899b6103c7f9a5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Padding the sequences \n",
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ],
   "id": "4965c995e170ea15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Truncate the sequences\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ],
   "id": "d240df9e5ee70c72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#tensors\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ],
   "id": "93dc0f02f7d0a2bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Special tokens\n",
    "The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don’t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you."
   ],
   "id": "8f7afcbcd33470c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T18:34:21.092194Z",
     "start_time": "2024-08-14T18:34:21.075953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#special tokens\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "#decode\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ],
   "id": "e2c8153a48fb01d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Wrapping up: From tokenizer to model",
   "id": "432ad2cb28372fdc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T18:43:52.290326Z",
     "start_time": "2024-08-14T18:43:51.964548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(output.logits)"
   ],
   "id": "b531e663bd7ab4d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f72d6fcc296cfd7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
